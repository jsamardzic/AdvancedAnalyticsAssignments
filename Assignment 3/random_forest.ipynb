{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e40ea18",
   "metadata": {},
   "source": [
    "# Developing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8e4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n",
    "from pyspark.sql import SparkSession # to initiate spark\n",
    "from pyspark.ml.linalg import Vectors # to allow us to work with VectorAssembler\n",
    "from pyspark.ml.feature import VectorAssembler # to combine our feature columns to pass to LogisticRegression model\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, IDFModel # vectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel # logisric regression ml model\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel # random forest ml model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator # to evaluate the model\n",
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e1adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:42:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AABDW Assignment 3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11fdfc",
   "metadata": {},
   "source": [
    "# Featurising  _'Title'_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291000a",
   "metadata": {},
   "source": [
    "Do all the subsetting _*before*_ using spark, otherwise we will use way too much memory and it will crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccae520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in the dataset as pandas dataframe\n",
    "df = pd.read_csv(\"assignment3_full.csv\")\n",
    "## turn our label into a binary variable\n",
    "df['frontpage'] = df['frontpage'].astype(int)\n",
    "## use a sample of the data to use less memory\n",
    "# df = df.sample(frac=0.1, random_state=1)\n",
    "## subset by label and variable we wish to featurise, to use less memory\n",
    "df = df[['frontpage', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e144ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 14684\n",
      "Test Dataset Count: 3672\n"
     ]
    }
   ],
   "source": [
    "## Split Train/Test data\n",
    "train, test= train_test_split(df, test_size=0.2, random_state=1)\n",
    "print(\"Training Dataset Count: \" + str(train.shape[0]))\n",
    "print(\"Test Dataset Count: \" + str(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d0dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a spark DataFrame out of our pandas DataFrame\n",
    "spark_train = spark.createDataFrame(train)\n",
    "spark_test = spark.createDataFrame(test)\n",
    "#spark_df = spark_df.sample(withReplacement=False, fraction=0.1, seed=1)\n",
    "## repartition the df (may make it easier to process)\n",
    "spark_train = spark_train.repartition(4)\n",
    "spark_test = spark_test.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf0c45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## convert our variable of text into list of words\n",
    "tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_words\", pattern=\"\\\\W\")\n",
    "\n",
    "## adds a column of our tokenised words to df\n",
    "spark_train_tokenised = tokenizer.transform(spark_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97616207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## stopwords remove to remove common, uninformative words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"f_title_words\")\n",
    "\n",
    "## adds a column of the filtered words to df\n",
    "spark_train_stopwordless = stopwords_remover.transform(spark_train_tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d62f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate term frequency in each article\n",
    "hashing_tf = HashingTF(inputCol=\"f_title_words\",\n",
    "                       outputCol=\"raw_features\", \n",
    "                       numFeatures=256)\n",
    "\n",
    "## adds raw tf features to the DF\n",
    "featurized_data = hashing_tf.transform(spark_train_stopwordless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d63c61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"title_features\")\n",
    "\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "## converting text to vectors\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94518cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['title_features'],outputCol=\"features\")\n",
    "rescaled_data = assembler.transform(rescaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd542f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:44:13 WARN DAGScheduler: Broadcasting large task binary with size 1098.4 KiB\n",
      "24/05/25 17:44:13 WARN DAGScheduler: Broadcasting large task binary with size 1744.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## model object\n",
    "rf = RandomForestClassifier(featuresCol='features',\n",
    "                            labelCol='frontpage',\n",
    "                            numTrees=500)\n",
    "\n",
    "## train model with default parameters\n",
    "rfmodel = rf.fit(rescaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67bff9",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2e1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline staging area\n",
    "tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_words\", pattern=\"\\\\W\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"title_words\", outputCol=\"f_title_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"f_title_words\",\n",
    "                       outputCol=\"raw_features\", \n",
    "                       numFeatures=256)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"title_features\")\n",
    "assembler = VectorAssembler(inputCols=['title_features'],outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol='features',\n",
    "                            labelCol='frontpage',\n",
    "                            numTrees=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a42ccf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d70fa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:44:39 WARN DAGScheduler: Broadcasting large task binary with size 1098.4 KiB\n",
      "24/05/25 17:44:40 WARN DAGScheduler: Broadcasting large task binary with size 1744.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model=pipeline.fit(spark_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c42278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:44:46 WARN TaskSetManager: Stage 86 contains a task of very large size (1024 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/Jovan/Library/Mobile Documents/com~apple~CloudDocs/Uni/Semester2/AdvancedAnalyticsInBusiness/Project/spark/project\"\n",
    "model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad06d712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/Jovan/Library/Mobile%20Documents/com~apple~CloudDocs/Uni/Semester2/AdvancedAnalyticsInBusiness/Project/spark/spark-3.5.1-bin-hadoop3/jars/spark-core_2.12-3.5.1.jar) to field java.math.BigInteger.mag\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "pred_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c690346",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred_model.transform(spark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "334b1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+---------+\n",
      "|               title|         probability|prediction|frontpage|\n",
      "+--------------------+--------------------+----------+---------+\n",
      "|             Pile-T5|[0.82440822642678...|       0.0|        1|\n",
      "|Firefox: Apply fo...|[0.82265626497984...|       0.0|        0|\n",
      "|US economic growt...|[0.82647763156330...|       0.0|        0|\n",
      "|Novelists as Scho...|[0.82598951651131...|       0.0|        0|\n",
      "|Show HN: I made a...|[0.85085676968993...|       0.0|        0|\n",
      "|No Abstractions: ...|[0.82755194547151...|       0.0|        1|\n",
      "|The Devil Went Do...|[0.82760571283405...|       0.0|        0|\n",
      "|Show HN: 1/Month ...|[0.84903911295391...|       0.0|        0|\n",
      "|Unraveling life's...|[0.82283474567546...|       0.0|        1|\n",
      "|Show HN: Roast my...|[0.85308268615294...|       0.0|        0|\n",
      "|'To the Future': ...|[0.81325033273763...|       0.0|        0|\n",
      "|Finding the Balan...|[0.82547392246384...|       0.0|        0|\n",
      "|Speculative decod...|[0.82592644664475...|       0.0|        0|\n",
      "|Lucky vs. Repeatable|[0.82704197093406...|       0.0|        1|\n",
      "|Show HN: Rapidx B...|[0.84851894810993...|       0.0|        0|\n",
      "|Natural sorting o...|[0.82577795480268...|       0.0|        0|\n",
      "|RecurrentGemma: M...|[0.82689479010944...|       0.0|        1|\n",
      "|Ruwiki (Wikipedia...|[0.82818272157296...|       0.0|        1|\n",
      "|Show HN: PodBear ...|[0.84927639750653...|       0.0|        0|\n",
      "|What's Happening ...|[0.82925962405500...|       0.0|        0|\n",
      "+--------------------+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:44:52 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"title\", \"probability\", \"prediction\", \"frontpage\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "862ad2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 17:44:56 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set Accuracy is :  0.5799212050626649\n"
     ]
    }
   ],
   "source": [
    "## to evalute model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"frontpage\")\n",
    "\n",
    "## print test accuracy\n",
    "print(\"Test-set Accuracy is : \", evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
